{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c71c78b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-27T12:03:56.808102Z",
     "iopub.status.busy": "2022-01-27T12:03:56.804272Z",
     "iopub.status.idle": "2022-01-27T12:04:03.813997Z",
     "shell.execute_reply": "2022-01-27T12:04:03.815189Z",
     "shell.execute_reply.started": "2022-01-27T11:49:33.524056Z"
    },
    "papermill": {
     "duration": 7.028867,
     "end_time": "2022-01-27T12:04:03.815462",
     "exception": false,
     "start_time": "2022-01-27T12:03:56.786595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'wsdr'...\r\n",
      "remote: Enumerating objects: 54, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (48/48), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\r\n",
      "remote: Total 54 (delta 12), reused 48 (delta 12), pack-reused 6\u001b[K\r\n",
      "Unpacking objects: 100% (54/54), 46.81 KiB | 538.00 KiB/s, done.\r\n",
      "/kaggle/working/wsdr\n",
      "Fetching origin\r\n",
      "HEAD is now at 43e204d Add global term\r\n",
      "From https://github.com/VietHoang1512/wsdr\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "! git clone https://VietHoang1512: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@github.com/VietHoang1512/wsdr.git\n",
    "! git config --global user.email \"phanviethoang1512@gmail.com\"\n",
    "! git config --global user.name \"VietHoang1512\"\n",
    "%cd wsdr\n",
    "! git fetch --all\n",
    "! git reset --hard origin/main\n",
    "! git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0147716e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T12:04:03.846106Z",
     "iopub.status.busy": "2022-01-27T12:04:03.844402Z",
     "iopub.status.idle": "2022-01-27T12:04:03.851801Z",
     "shell.execute_reply": "2022-01-27T12:04:03.850973Z",
     "shell.execute_reply.started": "2022-01-27T11:59:11.782525Z"
    },
    "papermill": {
     "duration": 0.025367,
     "end_time": "2022-01-27T12:04:03.852007",
     "exception": false,
     "start_time": "2022-01-27T12:04:03.826640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime_semi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime_semi.py\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from itertools import cycle\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.models import get_model, get_optimizer\n",
    "from svgd.trades_svgd_supsem_new import trades_svgd_supsem_loss, trades_svgd_ot_supsem_loss\n",
    "from utils.accuracy import accuracy\n",
    "from utils.datasets import get_data_loader, get_supsem_settings\n",
    "from utils.vat_new import vat_loss\n",
    "from potential_net import KanNet\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def train(model, kannet, label_x, label_y, unlab_x, optimizer, kannet_optimizer, epoch):\n",
    "    loss_ws = torch.zeros(1)\n",
    "    if opt.method == \"trades_svgd\":\n",
    "        loss_natural, loss_robust, loss, _ = trades_svgd_supsem_loss(\n",
    "            model,\n",
    "            label_x,\n",
    "            label_y,\n",
    "            unlab_x,\n",
    "            optimizer,\n",
    "            n=opt.n,\n",
    "            sigma=None,\n",
    "            xi=params[\"xi\"],\n",
    "            eps=params[\"epsilon\"],\n",
    "            niter=params[\"perturb_steps\"],\n",
    "            beta=params[\"beta\"],\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    if opt.method == \"trades_svgd_ot\":\n",
    "        loss_natural, loss_robust, loss_ws, loss  = trades_svgd_ot_supsem_loss(\n",
    "            model,\n",
    "            kannet,\n",
    "            label_x,\n",
    "            label_y,\n",
    "            unlab_x,\n",
    "            optimizer,\n",
    "            kannet_optimizer,\n",
    "            n=opt.n,\n",
    "            sigma=None,\n",
    "            xi=params[\"xi\"],\n",
    "            eps=params[\"epsilon\"],\n",
    "            niter=params[\"perturb_steps\"],\n",
    "            beta=params[\"beta\"],\n",
    "            gamma=params[\"gamma\"],\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    elif opt.method == \"vat\":\n",
    "        # vat_loss = VATLoss(xi=10.0, eps=params[\"epsilon\"], ip=1, beta=params[\"beta\"])\n",
    "        loss_natural, loss_robust, loss, x_adv = vat_loss(\n",
    "            model,\n",
    "            label_x,\n",
    "            label_y,\n",
    "            unlab_x,\n",
    "            optimizer,\n",
    "            n=opt.n,\n",
    "            xi=params[\"xi\"],\n",
    "            eps=params[\"epsilon\"],\n",
    "            niter=params[\"perturb_steps\"],\n",
    "            beta=params[\"beta\"],\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    else:\n",
    "        loss_robust = torch.tensor(0.0).detach()\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "        y_pred = model(label_x)\n",
    "        loss_natural = ce(y_pred, label_y)\n",
    "        loss = loss_natural\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_natural, loss_robust, loss_ws\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='cifar10')\n",
    "parser.add_argument('--model', type=str, default='cnn13', help='cnn|resnet')\n",
    "\n",
    "parser.add_argument('--num_epochs', type=int, default=1)\n",
    "parser.add_argument('--num_label', type=int, default=4000)\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--trial', type=int, default=2, help='4000')\n",
    "parser.add_argument('--gpu_id', type=str, default='1')\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument(\"--beta\", type=float, default=10.0, help=\"wdr weight\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.1, help=\"ws weight\")\n",
    "parser.add_argument(\"--n\", type=int, default=4, help=\"num particles\")\n",
    "\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "seed_everything(opt.seed)\n",
    "dataset = \"cifar10\"\n",
    "# attacks = [\"vat\", \"trades_svgd\"]\n",
    "\n",
    "attacks = [\"trades_svgd_ot\"]\n",
    "opt.dataset = dataset\n",
    "\n",
    "particles = [1,2,3,4]\n",
    "model = \"cnn13\"\n",
    "\n",
    "opt.model = model\n",
    "for attack in attacks:\n",
    "    opt.method = attack\n",
    "    for n in particles:\n",
    "\n",
    "        opt.n = n\n",
    "\n",
    "        params = get_supsem_settings(opt.dataset, opt.method)\n",
    "        params[\"num_label\"] = opt.num_label\n",
    "        label_loader, unlab_loader, test_loader = get_data_loader(ds=opt.dataset, batch_size=opt.batch_size, num_label = opt.num_label, kind=\"semi\")\n",
    "\n",
    "        params[\"beta\"] = opt.beta\n",
    "        params[\"gamma\"] = opt.gamma\n",
    "\n",
    "        model =  get_model(ds=opt.dataset, model=opt.model, activation=nn.ReLU())\n",
    "        model.cuda()\n",
    "        kannet = KanNet(model.latent_dim).cuda()\n",
    "        optimizer, lr = get_optimizer(ds=opt.dataset, model=model, architecture=opt.model)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                        T_max=opt.num_epochs,\n",
    "                                                        eta_min=0.0001)\n",
    "        \n",
    "        \n",
    "        kannet_optimizer = torch.optim.SGD(kannet.parameters(), lr=.1)\n",
    "        kannet_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            kannet_optimizer, T_max=opt.num_epochs, eta_min=0.0001\n",
    "        )\n",
    "        print(\"#\"*10)\n",
    "        start_time = datetime.now()\n",
    "        scheduler.step()\n",
    "        kannet_scheduler.step()\n",
    "        total_ws = 0.0\n",
    "        total_robust = 0.0\n",
    "        total_natural = 0.0\n",
    "        for i, ((label_x, label_y), (unlab_x, unlab_y)) in enumerate(\n",
    "            zip(cycle(label_loader), unlab_loader)\n",
    "        ):\n",
    "            label_x, label_y = label_x.cuda(), label_y.cuda()\n",
    "            unlab_x, unlab_y = unlab_x.cuda(), unlab_y.cuda()\n",
    "\n",
    "            train(\n",
    "                model, kannet, label_x, label_y, unlab_x, optimizer, kannet_optimizer, 1\n",
    "            )\n",
    "\n",
    "        batch_runtime = datetime.now()-start_time \n",
    "        total_time = batch_runtime.total_seconds()\n",
    "        print(opt.dataset, opt.method, opt.model, opt.n ,total_time, total_time/((i+1)*opt.batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf19573f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T12:04:03.874240Z",
     "iopub.status.busy": "2022-01-27T12:04:03.873311Z",
     "iopub.status.idle": "2022-01-27T12:15:25.782316Z",
     "shell.execute_reply": "2022-01-27T12:15:25.781700Z"
    },
    "papermill": {
     "duration": 681.921076,
     "end_time": "2022-01-27T12:15:25.782481",
     "exception": false,
     "start_time": "2022-01-27T12:04:03.861405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv.weight torch.Size([128, 3, 3, 3])\r\n",
      "layer1.0.conv.bias torch.Size([128])\r\n",
      "layer1.0.bn.weight torch.Size([128])\r\n",
      "layer1.0.bn.bias torch.Size([128])\r\n",
      "layer1.1.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.1.conv.bias torch.Size([128])\r\n",
      "layer1.1.bn.weight torch.Size([128])\r\n",
      "layer1.1.bn.bias torch.Size([128])\r\n",
      "layer1.2.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.2.conv.bias torch.Size([128])\r\n",
      "layer1.2.bn.weight torch.Size([128])\r\n",
      "layer1.2.bn.bias torch.Size([128])\r\n",
      "layer2.0.conv.weight torch.Size([256, 128, 3, 3])\r\n",
      "layer2.0.conv.bias torch.Size([256])\r\n",
      "layer2.0.bn.weight torch.Size([256])\r\n",
      "layer2.0.bn.bias torch.Size([256])\r\n",
      "layer2.1.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.1.conv.bias torch.Size([256])\r\n",
      "layer2.1.bn.weight torch.Size([256])\r\n",
      "layer2.1.bn.bias torch.Size([256])\r\n",
      "layer2.2.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.2.conv.bias torch.Size([256])\r\n",
      "layer2.2.bn.weight torch.Size([256])\r\n",
      "layer2.2.bn.bias torch.Size([256])\r\n",
      "layer3.0.conv.weight torch.Size([512, 256, 3, 3])\r\n",
      "layer3.0.conv.bias torch.Size([512])\r\n",
      "layer3.0.bn.weight torch.Size([512])\r\n",
      "layer3.0.bn.bias torch.Size([512])\r\n",
      "layer3.1.conv.weight torch.Size([256, 512, 1, 1])\r\n",
      "layer3.1.conv.bias torch.Size([256])\r\n",
      "layer3.1.bn.weight torch.Size([256])\r\n",
      "layer3.1.bn.bias torch.Size([256])\r\n",
      "layer3.2.conv.weight torch.Size([128, 256, 1, 1])\r\n",
      "layer3.2.conv.bias torch.Size([128])\r\n",
      "layer3.2.bn.weight torch.Size([128])\r\n",
      "layer3.2.bn.bias torch.Size([128])\r\n",
      "fc1.weight torch.Size([64, 128])\r\n",
      "fc1.bias torch.Size([64])\r\n",
      "fc2.weight torch.Size([10, 64])\r\n",
      "fc2.bias torch.Size([10])\r\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\r\n",
      "170499072it [00:05, 30117877.61it/s]                                            \r\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\r\n",
      "Files already downloaded and verified\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  cpuset_checked))\r\n",
      "##########\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\r\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\r\n",
      "cifar10 trades_svgd_ot cnn13 1 92.942998 0.0018618389022435897\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 2 135.448343 0.0027133081530448717\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 3 186.219766 0.0037303639022435897\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 4 240.890048 0.004825521794871795\r\n"
     ]
    }
   ],
   "source": [
    "! python runtime_semi.py --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7846f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T12:15:25.856909Z",
     "iopub.status.busy": "2022-01-27T12:15:25.855892Z",
     "iopub.status.idle": "2022-01-27T12:26:31.034408Z",
     "shell.execute_reply": "2022-01-27T12:26:31.033784Z"
    },
    "papermill": {
     "duration": 665.21778,
     "end_time": "2022-01-27T12:26:31.034590",
     "exception": false,
     "start_time": "2022-01-27T12:15:25.816810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv.weight torch.Size([128, 3, 3, 3])\r\n",
      "layer1.0.conv.bias torch.Size([128])\r\n",
      "layer1.0.bn.weight torch.Size([128])\r\n",
      "layer1.0.bn.bias torch.Size([128])\r\n",
      "layer1.1.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.1.conv.bias torch.Size([128])\r\n",
      "layer1.1.bn.weight torch.Size([128])\r\n",
      "layer1.1.bn.bias torch.Size([128])\r\n",
      "layer1.2.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.2.conv.bias torch.Size([128])\r\n",
      "layer1.2.bn.weight torch.Size([128])\r\n",
      "layer1.2.bn.bias torch.Size([128])\r\n",
      "layer2.0.conv.weight torch.Size([256, 128, 3, 3])\r\n",
      "layer2.0.conv.bias torch.Size([256])\r\n",
      "layer2.0.bn.weight torch.Size([256])\r\n",
      "layer2.0.bn.bias torch.Size([256])\r\n",
      "layer2.1.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.1.conv.bias torch.Size([256])\r\n",
      "layer2.1.bn.weight torch.Size([256])\r\n",
      "layer2.1.bn.bias torch.Size([256])\r\n",
      "layer2.2.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.2.conv.bias torch.Size([256])\r\n",
      "layer2.2.bn.weight torch.Size([256])\r\n",
      "layer2.2.bn.bias torch.Size([256])\r\n",
      "layer3.0.conv.weight torch.Size([512, 256, 3, 3])\r\n",
      "layer3.0.conv.bias torch.Size([512])\r\n",
      "layer3.0.bn.weight torch.Size([512])\r\n",
      "layer3.0.bn.bias torch.Size([512])\r\n",
      "layer3.1.conv.weight torch.Size([256, 512, 1, 1])\r\n",
      "layer3.1.conv.bias torch.Size([256])\r\n",
      "layer3.1.bn.weight torch.Size([256])\r\n",
      "layer3.1.bn.bias torch.Size([256])\r\n",
      "layer3.2.conv.weight torch.Size([128, 256, 1, 1])\r\n",
      "layer3.2.conv.bias torch.Size([128])\r\n",
      "layer3.2.bn.weight torch.Size([128])\r\n",
      "layer3.2.bn.bias torch.Size([128])\r\n",
      "fc1.weight torch.Size([64, 128])\r\n",
      "fc1.bias torch.Size([64])\r\n",
      "fc2.weight torch.Size([10, 64])\r\n",
      "fc2.bias torch.Size([10])\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  cpuset_checked))\r\n",
      "##########\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\r\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\r\n",
      "cifar10 trades_svgd_ot cnn13 1 88.357755 0.001769987079326923\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 2 135.099257 0.0027063152443910256\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 3 186.928842 0.0037445681490384615\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 4 240.822562 0.004824169911858974\r\n"
     ]
    }
   ],
   "source": [
    "! python runtime_semi.py --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9a9ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T12:26:31.121393Z",
     "iopub.status.busy": "2022-01-27T12:26:31.120450Z",
     "iopub.status.idle": "2022-01-27T12:37:35.969185Z",
     "shell.execute_reply": "2022-01-27T12:37:35.968563Z"
    },
    "papermill": {
     "duration": 664.894005,
     "end_time": "2022-01-27T12:37:35.969337",
     "exception": false,
     "start_time": "2022-01-27T12:26:31.075332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv.weight torch.Size([128, 3, 3, 3])\r\n",
      "layer1.0.conv.bias torch.Size([128])\r\n",
      "layer1.0.bn.weight torch.Size([128])\r\n",
      "layer1.0.bn.bias torch.Size([128])\r\n",
      "layer1.1.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.1.conv.bias torch.Size([128])\r\n",
      "layer1.1.bn.weight torch.Size([128])\r\n",
      "layer1.1.bn.bias torch.Size([128])\r\n",
      "layer1.2.conv.weight torch.Size([128, 128, 3, 3])\r\n",
      "layer1.2.conv.bias torch.Size([128])\r\n",
      "layer1.2.bn.weight torch.Size([128])\r\n",
      "layer1.2.bn.bias torch.Size([128])\r\n",
      "layer2.0.conv.weight torch.Size([256, 128, 3, 3])\r\n",
      "layer2.0.conv.bias torch.Size([256])\r\n",
      "layer2.0.bn.weight torch.Size([256])\r\n",
      "layer2.0.bn.bias torch.Size([256])\r\n",
      "layer2.1.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.1.conv.bias torch.Size([256])\r\n",
      "layer2.1.bn.weight torch.Size([256])\r\n",
      "layer2.1.bn.bias torch.Size([256])\r\n",
      "layer2.2.conv.weight torch.Size([256, 256, 3, 3])\r\n",
      "layer2.2.conv.bias torch.Size([256])\r\n",
      "layer2.2.bn.weight torch.Size([256])\r\n",
      "layer2.2.bn.bias torch.Size([256])\r\n",
      "layer3.0.conv.weight torch.Size([512, 256, 3, 3])\r\n",
      "layer3.0.conv.bias torch.Size([512])\r\n",
      "layer3.0.bn.weight torch.Size([512])\r\n",
      "layer3.0.bn.bias torch.Size([512])\r\n",
      "layer3.1.conv.weight torch.Size([256, 512, 1, 1])\r\n",
      "layer3.1.conv.bias torch.Size([256])\r\n",
      "layer3.1.bn.weight torch.Size([256])\r\n",
      "layer3.1.bn.bias torch.Size([256])\r\n",
      "layer3.2.conv.weight torch.Size([128, 256, 1, 1])\r\n",
      "layer3.2.conv.bias torch.Size([128])\r\n",
      "layer3.2.bn.weight torch.Size([128])\r\n",
      "layer3.2.bn.bias torch.Size([128])\r\n",
      "fc1.weight torch.Size([64, 128])\r\n",
      "fc1.bias torch.Size([64])\r\n",
      "fc2.weight torch.Size([10, 64])\r\n",
      "fc2.bias torch.Size([10])\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  cpuset_checked))\r\n",
      "##########\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\r\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\r\n",
      "cifar10 trades_svgd_ot cnn13 1 88.932777 0.0017815059495192309\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 2 136.012375 0.0027246068709935894\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 3 185.718359 0.00372031969150641\r\n",
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "##########\r\n",
      "cifar10 trades_svgd_ot cnn13 4 240.630348 0.004820319471153846\r\n"
     ]
    }
   ],
   "source": [
    "! python runtime_semi.py --seed 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2029.229308,
   "end_time": "2022-01-27T12:37:36.429851",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-27T12:03:47.200543",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
