# -*- coding: utf-8 -*-
"""02D_adversarial_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12zvavW3DUyxikEIsMHq_e2QgqeH_0CRF
"""

"""
    Adversarial Training for MNIST and CIFAR dataset
"""
import numpy as np
import torch 
import torch.nn as nn 
import torch.nn.functional as F 
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.optim as optim
from torch.autograd import Variable
import matplotlib as mpl
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from torch.utils.tensorboard import SummaryWriter

from mysetting import * 
from models import activ, get_model, adjust_learning_rate, get_optimizer, DataWithIndex
from utils_cm import mkdir_p, writelog, backup
from mytrain import test, adv_test
from mytrain import baseline_train
from mytrain import svgd_train
from time import gmtime, strftime
import time 
#------------------------------------------------------
# Dataset preprocessing 
if args.ds == 'mnist':
    nb_classes = 10
    x_max = 1.
    x_min = 0.
    transform=transforms.Compose([
            transforms.ToTensor(),
            ])
    train_data = datasets.MNIST('../data', train=True, download=True,
                        transform=transform)
    test_data = datasets.MNIST('../data', train=False,
                        transform=transform)
    epsilon = 0.3 
    step_size = 0.01
    num_steps= 20
    epsilon_range = [0.1, 0.2, 0.25, 0.3, 0.325, 0.35, 0.375, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.6, 0.7]

elif args.ds == 'cifar10': 
    nb_classes = 10
    x_max = 1.
    x_min = 0.
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])
    train_data = datasets.CIFAR10('../data', train=True, download=True,
                        transform=transform_train)
    test_data = datasets.CIFAR10('../data', train=False,
                        transform=transform_test)    

    epsilon = 0.031
    step_size = 0.007
    num_steps= 10
    epsilon_range = [8., 10., 12., 14., 16., 20.]
    epsilon_range = [x/255 for x in epsilon_range]

elif args.ds == 'cifar100': 
    nb_classes = 100
    x_max = 1.
    x_min = 0.
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])
    train_data = datasets.CIFAR100('../data', train=True, download=True,
                        transform=transform_train)
    test_data = datasets.CIFAR100('../data', train=False,
                        transform=transform_test)    

    epsilon = 0.031 #0.01
    step_size = 0.007 #0.001
    num_steps= 10
    epsilon_range = [8., 10., 12., 14., 16., 20.]
    epsilon_range = [x/8*epsilon for x in epsilon_range]

#------------------------------------------------------
# Params setting 
log_interval = 10

attack_params = dict()

attack_params['attack_type'] = args.attack_type
attack_params['epsilon'] = args.epsilon
attack_params['step_size'] = step_size if args.step_size < 0 else args.step_size
attack_params['num_steps'] = num_steps
attack_params['x_min'] = x_min
attack_params['x_max'] = x_max

attack_params['defense'] = args.defense
attack_params['order'] = int(args.order) if args.order != 'inf' else np.inf
attack_params['loss_type'] = args.loss_type
attack_params['random_init'] = args.random_init
attack_params['projecting'] = args.projecting
attack_params['alpha'] = args.alpha
attack_params['trades_beta'] = args.trades_beta
attack_params['wcom'] = args.wcom
attack_params['wkl'] = args.wkl
attack_params['sigma'] = None 
attack_params['num_particles'] = args.num_particles
attack_params['dist'] = args.dist

eval_params = attack_params.copy()
eval_params['num_steps'] = 10
eval_params['epsilon'] = epsilon
# ------------------------------------------------------
WP = './'
save_dir = WP + basedir + '/' + modeldir + '/'
mkdir_p(basedir)
mkdir_p(save_dir)
mkdir_p(save_dir+'/codes/')
backup('./', save_dir+'/codes/')
model_dir = save_dir + 'model.pt'
model_best_dir = save_dir + 'model_best.pt'
model_lamda_dir = save_dir + 'model_lamda.pt'
logfile = save_dir + 'log.txt'
writer = SummaryWriter(save_dir+'log/')

for key in attack_params.keys(): 
    writelog('attack_params, {}:{}'.format(key, attack_params[key]), logfile)

use_cuda = not args.no_cuda and torch.cuda.is_available()

np.random.seed(args.seed)
torch.manual_seed(args.seed)

device = torch.device("cuda" if use_cuda else "cpu")

train_kwargs = {'batch_size': args.bs, 'shuffle': True} #'drop_last': True
test_kwargs = {'batch_size': args.bs}
if use_cuda:
    cuda_kwargs = {'num_workers': 1,
                    'pin_memory': True,
                    }
    train_kwargs.update(cuda_kwargs)
    test_kwargs.update(cuda_kwargs)

#------------------------------------------------------
# Load dataset 
train_loader = torch.utils.data.DataLoader(train_data, **train_kwargs)
test_loader = torch.utils.data.DataLoader(test_data, **test_kwargs)    

#------------------------------------------------------
# Model 
model = get_model(args.ds, args.model, activation=activ(args.activ))
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  model = nn.DataParallel(model)

model.to(device)

opt, lr = get_optimizer(ds=args.ds, model=model, architecture=args.model)
#------------------------------------------------------
# Train model 
pre_acc = -1. 

if args.defense in ['pgd_train', 'trades_train', 'mart_train']:
    train_method = baseline_train
elif args.defense in ['pgd_svgd_symkl', 'pgd_svgd_ce', 'pgd_svgd_kl']: 
    train_method = svgd_train

start_time = time.time()
writelog(strftime("%Y-%m-%d %H:%M:%S", gmtime()), logfile)
for epoch in range(args.epochs): 
    opt = adjust_learning_rate(opt, epoch, lr=lr, ds=args.ds)
    writer = train_method(model, train_loader, epoch, opt, device, log_interval, attack_params, writer)

    nat_acc = test(model, test_loader, device)
    writer.add_scalar('nat_acc_test', nat_acc, epoch)
    if epoch % args.log_period == 0 and epoch > 0:
        adv_acc = adv_test(model, test_loader, device, eval_params)
        test_time = time.time()
        writelog('time:{:.0f}, epoch:{}, nat_acc:{}, adv_acc:{}'.format(test_time-start_time, epoch, nat_acc, adv_acc), logfile)
        writer.add_scalar('adv_acc_test', adv_acc, epoch)
        if adv_acc >= pre_acc: 
            pre_acc = adv_acc 
            torch.save(model.state_dict(), model_best_dir)
    else: 
        writelog('epoch:{}, nat_acc:{}'.format(epoch, nat_acc), logfile)
    torch.save(model.state_dict(), model_dir)
    if epoch % args.save_freq == 0:
        torch.save(model.state_dict(),
                    os.path.join(save_dir, 'model-nn-epoch{}.pt'.format(epoch)))
    writer.flush()
writer.close()   


nat_acc = test(model, test_loader, device)
adv_acc = adv_test(model, test_loader, device, eval_params)
writelog('nat_acc={:.4f}, adv_acc={:.4f}'.format(nat_acc, adv_acc), logfile)


#------------------------------------------------------
model_dir = model_best_dir
model.load_state_dict(torch.load(model_dir))
model.eval()
eval_params = attack_params.copy()
eval_params['num_steps'] = 200
eval_params['epsilon'] = epsilon

#------------------------------------------------------
writelog('----------EVAL STANDARD PGD-200 ----------------', logfile)
writelog('model_dir:{}'.format(model_dir), logfile)
nat_acc = test(model, test_loader, device)
adv_acc = adv_test(model, test_loader, device, eval_params)
writelog('--------------------------', logfile)
for key in eval_params.keys(): 
    writelog('eval_params, {}:{}'.format(key, eval_params[key]), logfile)
writelog('nat_acc={:.4f}, adv_acc={:.4f}'.format(nat_acc, adv_acc), logfile)

#------------------------------------------------------
if args.eval_auto: 
    from autoattack import AutoAttack
    writelog('----------EVAL AUTO ATTACK, Standard, Norm Linf ----------------', logfile)
    writelog('model_dir:{}'.format(model_dir), logfile)
    l = [x for (x, y) in test_loader]
    x_test = torch.cat(l, 0)
    l = [y for (x, y) in test_loader]
    y_test = torch.cat(l, 0)
    x_test = x_test[:10000]
    y_test = y_test[:10000]

    adversary = AutoAttack(model, norm='Linf', eps=epsilon, log_path=logfile, version='standard') 
    x_adv = adversary.run_standard_evaluation(x_test, y_test, bs=100)
    aa_data = TensorDataset(x_adv, y_test)
    aa_loader = DataLoader(aa_data, **test_kwargs)
    adv_acc = test(model, aa_loader, device)
    writelog('Auto-Attack, Standard, Linf, eps={}, adv_acc={:.4f}'.format(epsilon, adv_acc), logfile)

    # writelog('----------EVAL AUTO ATTACK, Standard, Norm L2 ----------------', logfile)
    # adversary = AutoAttack(model, norm='L2', eps=epsilon, log_path=logfile, version='standard') 
    # x_adv = adversary.run_standard_evaluation(x_test, y_test, bs=100)
    # aa_data = TensorDataset(x_adv, y_test)
    # aa_loader = DataLoader(aa_data, **test_kwargs)
    # adv_acc = test(model, aa_loader, device)
    # writelog('Auto-Attack, Standard, L2, eps={}, adv_acc={:.4f}'.format(epsilon, adv_acc), logfile)

#------------------------------------------------------
if args.eval_bb:
    import foolbox as fb
    writelog('----------EVAL B&B Attack ----------------', logfile)
    writelog('model_dir:{}'.format(model_dir), logfile)
    l = [x for (x, y) in test_loader]
    x_test = torch.cat(l, 0)
    l = [y for (x, y) in test_loader]
    y_test = torch.cat(l, 0)
    x_test = x_test[:10000]
    y_test = y_test[:10000]
    x_test = x_test.to(device)
    y_test = y_test.to(device)

    class init_attack(object):
        
        def __init__(self, attack):
            self.attack = attack
            
        def run(self, model, originals, criterion_):
            return self.attack(model, x_test, criterion=criterion_, epsilons=epsilon)[1]

    pdg_init_attack = fb.attacks.LinfPGD(steps=20, abs_stepsize=epsilon/2, random_start=True)
    bb_attack = fb.attacks.LinfinityBrendelBethgeAttack(init_attack(pdg_init_attack), steps=200)
    fmodel = fb.PyTorchModel(model, bounds=(0, 1))
    _, _, init_success = pdg_init_attack(fmodel, x_test, y_test, epsilons=epsilon)
    _, advs, success = bb_attack(fmodel, x_test, 
                                criterion=fb.criteria.Misclassification(y_test), 
                                epsilons=epsilon)
    init_acc = 1 - np.mean(init_success.cpu().detach().numpy())
    adv_acc = 1 - np.mean(success.cpu().detach().numpy())
    writelog('B&B Attack, init with PGD-20, eps={}, adv_acc of initial PGD attack={:.4f}'.format(epsilon, init_acc), logfile)
    writelog('B&B Attack, eps={}, adv_acc={:.4f}'.format(epsilon, adv_acc), logfile)

#------------------------------------------------------
if args.eval_multi:
    writelog('----------EVAL MULTIPLE EPSILONS ----------------', logfile)
    writelog('model_dir:{}'.format(model_dir), logfile)
    eval_multi = eval_params.copy()
    for eps in epsilon_range:
        eval_multi['epsilon'] = eps
        eval_multi['num_steps'] = 50 
        # nat_acc = test(model, test_loader, device)
        adv_acc = adv_test(model, test_loader, device, eval_multi)
        writelog('--------------------------', logfile)
        for key in eval_multi.keys(): 
            writelog('eval_params, {}:{}'.format(key, eval_multi[key]), logfile)
        writelog('nat_acc={:.4f}, adv_acc={:.4f}'.format(nat_acc, adv_acc), logfile)